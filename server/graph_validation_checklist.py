"""
Graph Intelligence Validation Checklist
SPEC-040/041 Optimization: Comprehensive validation and testing suite for graph reasoning
"""

import json
import time
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from uuid import uuid4

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from auth import get_current_user, get_db
from redis_client import get_redis_client
from graph_intelligence_integration_api import GraphIntelligenceEngine, get_intelligence_engine

# Initialize router
router = APIRouter(prefix="/graph-validation", tags=["graph-validation"])

@dataclass
class ValidationResult:
    """Result of a validation test"""
    test_name: str
    passed: bool
    score: float
    details: Dict[str, Any]
    execution_time_ms: int
    error_message: Optional[str] = None

@dataclass
class ValidationSuite:
    """Complete validation suite results"""
    suite_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    overall_score: float
    execution_time_ms: int
    results: List[ValidationResult]
    timestamp: datetime

class GraphValidationEngine:
    """Core engine for graph intelligence validation"""
    
    def __init__(self, intelligence_engine: GraphIntelligenceEngine):
        self.engine = intelligence_engine
        self.test_data = self._generate_test_data()
    
    def _generate_test_data(self) -> Dict[str, Any]:
        """Generate test data for validation"""
        return {
            "test_memories": [
                {"id": "mem_001", "title": "Python Programming", "content": "Learning Python basics", "type": "Memory"},
                {"id": "mem_002", "title": "Machine Learning", "content": "Introduction to ML algorithms", "type": "Memory"},
                {"id": "mem_003", "title": "Data Science", "content": "Data analysis with pandas", "type": "Memory"},
                {"id": "mem_004", "title": "Web Development", "content": "Building REST APIs", "type": "Memory"},
                {"id": "mem_005", "title": "Database Design", "content": "SQL and NoSQL databases", "type": "Memory"}
            ],
            "test_users": [
                {"id": "user_001", "name": "Alice Developer", "type": "User"},
                {"id": "user_002", "name": "Bob Analyst", "type": "User"},
                {"id": "user_003", "name": "Carol Manager", "type": "User"}
            ],
            "expected_similarities": {
                "mem_001": ["mem_002", "mem_003"],  # Python -> ML, Data Science
                "mem_002": ["mem_001", "mem_003"],  # ML -> Python, Data Science
                "mem_004": ["mem_005"],             # Web Dev -> Database
            },
            "expected_paths": [
                {"source": "mem_001", "target": "mem_003", "max_length": 3},
                {"source": "user_001", "target": "mem_001", "max_length": 2},
            ]
        }
    
    async def validate_similarity_analysis(self) -> ValidationResult:
        """Validate similarity analysis accuracy"""
        start_time = time.time()
        test_name = "similarity_analysis"
        
        try:
            passed_tests = 0
            total_tests = 0
            details = {"test_cases": [], "accuracy_scores": []}
            
            for source_id, expected_similar in self.test_data["expected_similarities"].items():
                total_tests += 1
                
                # Execute similarity query
                result = await self.engine.execute_graph_reasoning(
                    query_type="similarity",
                    entity_id=source_id,
                    entity_type="Memory",
                    parameters={"similarity_threshold": 0.5},
                    max_results=5
                )
                
                # Analyze results
                found_similar = [r["id"] for r in result["results"]]
                matches = len(set(expected_similar) & set(found_similar))
                accuracy = matches / len(expected_similar) if expected_similar else 1.0
                
                details["test_cases"].append({
                    "source": source_id,
                    "expected": expected_similar,
                    "found": found_similar,
                    "matches": matches,
                    "accuracy": accuracy
                })
                details["accuracy_scores"].append(accuracy)
                
                if accuracy >= 0.7:  # 70% accuracy threshold
                    passed_tests += 1
            
            overall_accuracy = sum(details["accuracy_scores"]) / len(details["accuracy_scores"])
            execution_time = int((time.time() - start_time) * 1000)
            
            return ValidationResult(
                test_name=test_name,
                passed=(passed_tests / total_tests) >= 0.8,  # 80% of tests must pass
                score=overall_accuracy,
                details=details,
                execution_time_ms=execution_time
            )
            
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ValidationResult(
                test_name=test_name,
                passed=False,
                score=0.0,
                details={"error": str(e)},
                execution_time_ms=execution_time,
                error_message=str(e)
            )
    
    async def validate_path_discovery(self) -> ValidationResult:
        """Validate path discovery functionality"""
        start_time = time.time()
        test_name = "path_discovery"
        
        try:
            passed_tests = 0
            total_tests = len(self.test_data["expected_paths"])
            details = {"test_cases": [], "path_scores": []}
            
            for path_test in self.test_data["expected_paths"]:
                # Execute path query
                result = await self.engine.execute_graph_reasoning(
                    query_type="path",
                    entity_id=path_test["source"],
                    entity_type="Memory",
                    parameters={
                        "target_id": path_test["target"],
                        "max_depth": path_test["max_length"]
                    },
                    max_results=3
                )
                
                # Analyze path results
                paths_found = len(result["results"])
                path_quality = 1.0 if paths_found > 0 else 0.0
                
                if result["results"]:
                    # Check path length and strength
                    best_path = max(result["results"], key=lambda p: p.get("strength", 0))
                    path_length = best_path.get("length", 999)
                    path_strength = best_path.get("strength", 0.0)
                    
                    # Score based on length and strength
                    length_score = 1.0 if path_length <= path_test["max_length"] else 0.5
                    strength_score = path_strength
                    path_quality = (length_score + strength_score) / 2
                
                details["test_cases"].append({
                    "source": path_test["source"],
                    "target": path_test["target"],
                    "expected_max_length": path_test["max_length"],
                    "paths_found": paths_found,
                    "path_quality": path_quality,
                    "results": result["results"][:2]  # First 2 results for inspection
                })
                details["path_scores"].append(path_quality)
                
                if path_quality >= 0.6:  # 60% quality threshold
                    passed_tests += 1
            
            overall_score = sum(details["path_scores"]) / len(details["path_scores"]) if details["path_scores"] else 0.0
            execution_time = int((time.time() - start_time) * 1000)
            
            return ValidationResult(
                test_name=test_name,
                passed=(passed_tests / total_tests) >= 0.7,  # 70% of tests must pass
                score=overall_score,
                details=details,
                execution_time_ms=execution_time
            )
            
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ValidationResult(
                test_name=test_name,
                passed=False,
                score=0.0,
                details={"error": str(e)},
                execution_time_ms=execution_time,
                error_message=str(e)
            )
    
    async def validate_recommendation_engine(self) -> ValidationResult:
        """Validate recommendation engine quality"""
        start_time = time.time()
        test_name = "recommendation_engine"
        
        try:
            passed_tests = 0
            total_tests = 0
            details = {"test_cases": [], "recommendation_scores": []}
            
            for memory in self.test_data["test_memories"][:3]:  # Test first 3 memories
                total_tests += 1
                
                # Execute recommendation query
                result = await self.engine.execute_graph_reasoning(
                    query_type="recommendation",
                    entity_id=memory["id"],
                    entity_type="Memory",
                    parameters={"recommendation_type": "content_based"},
                    max_results=3
                )
                
                # Analyze recommendation quality
                recommendations = result["results"]
                rec_count = len(recommendations)
                
                # Score based on number and quality of recommendations
                count_score = min(rec_count / 3, 1.0)  # Expect at least 3 recommendations
                
                if recommendations:
                    avg_confidence = sum(result["confidence_scores"].values()) / len(result["confidence_scores"])
                    confidence_score = avg_confidence
                else:
                    confidence_score = 0.0
                
                overall_rec_score = (count_score + confidence_score) / 2
                
                details["test_cases"].append({
                    "memory_id": memory["id"],
                    "memory_title": memory["title"],
                    "recommendations_count": rec_count,
                    "avg_confidence": confidence_score,
                    "overall_score": overall_rec_score,
                    "recommendations": recommendations[:2]  # First 2 for inspection
                })
                details["recommendation_scores"].append(overall_rec_score)
                
                if overall_rec_score >= 0.6:  # 60% quality threshold
                    passed_tests += 1
            
            overall_score = sum(details["recommendation_scores"]) / len(details["recommendation_scores"]) if details["recommendation_scores"] else 0.0
            execution_time = int((time.time() - start_time) * 1000)
            
            return ValidationResult(
                test_name=test_name,
                passed=(passed_tests / total_tests) >= 0.7,  # 70% of tests must pass
                score=overall_score,
                details=details,
                execution_time_ms=execution_time
            )
            
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ValidationResult(
                test_name=test_name,
                passed=False,
                score=0.0,
                details={"error": str(e)},
                execution_time_ms=execution_time,
                error_message=str(e)
            )
    
    async def validate_edge_weight_relevance(self) -> ValidationResult:
        """Validate edge weight decay, trust, and source relevance"""
        start_time = time.time()
        test_name = "edge_weight_relevance"
        
        try:
            # Test relationship strength calculation
            test_relationships = [
                {"source": "user_001", "target": "mem_001", "interaction_count": 10, "expected_strength": 0.8},
                {"source": "user_001", "target": "mem_002", "interaction_count": 5, "expected_strength": 0.6},
                {"source": "user_002", "target": "mem_003", "interaction_count": 1, "expected_strength": 0.3},
            ]
            
            passed_tests = 0
            total_tests = len(test_relationships)
            details = {"test_cases": [], "strength_scores": []}
            
            for rel in test_relationships:
                # Simulate relationship strength calculation
                # In production, this would query actual graph data
                calculated_strength = min(rel["interaction_count"] * 0.1, 1.0)  # Simple formula
                expected_strength = rel["expected_strength"]
                
                # Calculate accuracy
                accuracy = 1.0 - abs(calculated_strength - expected_strength)
                
                details["test_cases"].append({
                    "source": rel["source"],
                    "target": rel["target"],
                    "interaction_count": rel["interaction_count"],
                    "expected_strength": expected_strength,
                    "calculated_strength": calculated_strength,
                    "accuracy": accuracy
                })
                details["strength_scores"].append(accuracy)
                
                if accuracy >= 0.8:  # 80% accuracy threshold
                    passed_tests += 1
            
            overall_score = sum(details["strength_scores"]) / len(details["strength_scores"])
            execution_time = int((time.time() - start_time) * 1000)
            
            return ValidationResult(
                test_name=test_name,
                passed=(passed_tests / total_tests) >= 0.8,
                score=overall_score,
                details=details,
                execution_time_ms=execution_time
            )
            
        except Exception as e:
            execution_time = int((time.time() - start_time) * 1000)
            return ValidationResult(
                test_name=test_name,
                passed=False,
                score=0.0,
                details={"error": str(e)},
                execution_time_ms=execution_time,
                error_message=str(e)
            )
    
    async def run_full_validation_suite(self) -> ValidationSuite:
        """Run complete validation suite"""
        start_time = time.time()
        
        # Run all validation tests
        results = []
        
        print("üîç Running Graph Intelligence Validation Suite...")
        
        print("  ‚úì Testing similarity analysis...")
        similarity_result = await self.validate_similarity_analysis()
        results.append(similarity_result)
        
        print("  ‚úì Testing path discovery...")
        path_result = await self.validate_path_discovery()
        results.append(path_result)
        
        print("  ‚úì Testing recommendation engine...")
        recommendation_result = await self.validate_recommendation_engine()
        results.append(recommendation_result)
        
        print("  ‚úì Testing edge weight relevance...")
        edge_weight_result = await self.validate_edge_weight_relevance()
        results.append(edge_weight_result)
        
        # Calculate overall metrics
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.passed)
        failed_tests = total_tests - passed_tests
        overall_score = sum(r.score for r in results) / total_tests
        execution_time = int((time.time() - start_time) * 1000)
        
        return ValidationSuite(
            suite_name="Graph Intelligence Validation",
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            overall_score=overall_score,
            execution_time_ms=execution_time,
            results=results,
            timestamp=datetime.utcnow()
        )


# Pydantic Models for API
class ValidationRequest(BaseModel):
    """Request for validation testing"""
    test_types: List[str] = Field(default=["similarity", "path", "recommendation", "edge_weight"])
    include_details: bool = Field(default=True)
    cache_results: bool = Field(default=True)

class ValidationResponse(BaseModel):
    """Response from validation testing"""
    suite_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    overall_score: float
    execution_time_ms: int
    results: List[Dict[str, Any]]
    timestamp: str
    recommendations: List[str]

# Global validation engine
validation_engine = None

async def get_validation_engine(intelligence_engine: GraphIntelligenceEngine = Depends(get_intelligence_engine)):
    """Get or create validation engine instance"""
    global validation_engine
    if validation_engine is None:
        validation_engine = GraphValidationEngine(intelligence_engine)
    return validation_engine

# API Endpoints

@router.post("/run-suite", response_model=ValidationResponse)
async def run_validation_suite(
    request: ValidationRequest,
    current_user = Depends(get_current_user),
    validator: GraphValidationEngine = Depends(get_validation_engine)
):
    """Run comprehensive graph intelligence validation suite"""
    
    try:
        # Run validation suite
        suite_result = await validator.run_full_validation_suite()
        
        # Generate recommendations based on results
        recommendations = []
        for result in suite_result.results:
            if not result.passed:
                if result.test_name == "similarity_analysis":
                    recommendations.append("Consider tuning similarity thresholds and algorithms")
                elif result.test_name == "path_discovery":
                    recommendations.append("Optimize path finding algorithms and depth limits")
                elif result.test_name == "recommendation_engine":
                    recommendations.append("Improve recommendation scoring and filtering")
                elif result.test_name == "edge_weight_relevance":
                    recommendations.append("Adjust relationship strength calculation parameters")
            elif result.score < 0.8:
                recommendations.append(f"Fine-tune {result.test_name} for better performance")
        
        if not recommendations:
            recommendations.append("All tests passed! Graph intelligence system is performing well.")
        
        # Cache results if requested
        if request.cache_results:
            cache_key = f"graph:validation:{datetime.utcnow().strftime('%Y%m%d_%H')}"
            await validator.engine.redis.setex(
                cache_key, 
                3600,  # 1 hour
                json.dumps({
                    "suite_result": suite_result.__dict__,
                    "recommendations": recommendations
                }, default=str)
            )
        
        return ValidationResponse(
            suite_name=suite_result.suite_name,
            total_tests=suite_result.total_tests,
            passed_tests=suite_result.passed_tests,
            failed_tests=suite_result.failed_tests,
            overall_score=suite_result.overall_score,
            execution_time_ms=suite_result.execution_time_ms,
            results=[r.__dict__ for r in suite_result.results],
            timestamp=suite_result.timestamp.isoformat(),
            recommendations=recommendations
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Validation suite failed: {str(e)}")

@router.get("/health")
async def validation_health():
    """Health check for validation system"""
    return {
        "status": "healthy",
        "service": "graph-validation",
        "available_tests": ["similarity", "path", "recommendation", "edge_weight"],
        "timestamp": datetime.utcnow().isoformat()
    }

@router.get("/test-data")
async def get_test_data(
    current_user = Depends(get_current_user),
    validator: GraphValidationEngine = Depends(get_validation_engine)
):
    """Get test data used for validation"""
    return {
        "test_data": validator.test_data,
        "description": "Test data used for graph intelligence validation",
        "timestamp": datetime.utcnow().isoformat()
    }
