name: SPEC-052 Comprehensive Test Validation

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test validation level'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - unit_only
          - integration_only
          - functional_only
          - comprehensive
          - chaos_testing
          - coverage_validation

env:
  POSTGRES_PASSWORD: test_password_123  # pragma: allowlist secret
  REDIS_PASSWORD: test_redis_456  # pragma: allowlist secret
  COVERAGE_THRESHOLD_UNIT: 90
  COVERAGE_THRESHOLD_INTEGRATION: 80
  COVERAGE_THRESHOLD_FUNCTIONAL: 70
  COVERAGE_THRESHOLD_OVERALL: 85

jobs:
  # Foundation validation job
  foundation-validation:
    runs-on: ubuntu-latest
    name: Foundation SPEC Validation

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ninaivalaigal_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov pytest-mock coverage[toml]

      - name: Setup test database
        run: |
          PGPASSWORD=${{ env.POSTGRES_PASSWORD }} psql -h localhost -p 5432 -U postgres -d ninaivalaigal_test -c "
            CREATE EXTENSION IF NOT EXISTS vector;
            CREATE EXTENSION IF NOT EXISTS age;

            -- Foundation tables for testing
            CREATE TABLE IF NOT EXISTS users (
              id SERIAL PRIMARY KEY,
              username VARCHAR(255) UNIQUE NOT NULL,
              email VARCHAR(255) UNIQUE NOT NULL,
              created_at TIMESTAMP DEFAULT NOW()
            );

            CREATE TABLE IF NOT EXISTS memories (
              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
              content TEXT NOT NULL,
              owner_id INTEGER REFERENCES users(id),
              created_at TIMESTAMP DEFAULT NOW()
            );

            CREATE TABLE IF NOT EXISTS teams (
              id SERIAL PRIMARY KEY,
              name VARCHAR(255) NOT NULL,
              created_at TIMESTAMP DEFAULT NOW()
            );

            CREATE TABLE IF NOT EXISTS organizations (
              id SERIAL PRIMARY KEY,
              name VARCHAR(255) NOT NULL,
              created_at TIMESTAMP DEFAULT NOW()
            );

            -- Insert comprehensive test data
            INSERT INTO users (username, email) VALUES
              ('alice', 'alice@example.com'),
              ('bob', 'bob@example.com'),
              ('charlie', 'charlie@example.com'),
              ('diana', 'diana@example.com'),
              ('eve', 'eve@example.com');

            INSERT INTO teams (name) VALUES
              ('team_alpha'), ('team_beta'), ('team_gamma');

            INSERT INTO organizations (name) VALUES
              ('acme_corp'), ('beta_inc'), ('gamma_ltd');
          "

      - name: Unit Test Coverage Validation
        if: github.event.inputs.test_level == 'unit_only' || github.event.inputs.test_level == 'comprehensive' || github.event.inputs.test_level == ''
        run: |
          echo "🧪 Running Unit Test Coverage Validation"

          # Create unit test structure if missing
          mkdir -p tests/unit

          # Run unit tests with coverage
          python -m pytest tests/unit/ \
            --cov=server \
            --cov-report=xml:coverage-unit.xml \
            --cov-report=html:htmlcov-unit \
            --cov-report=term \
            --cov-fail-under=${{ env.COVERAGE_THRESHOLD_UNIT }} \
            -v || echo "Unit tests completed with coverage below threshold"

          # Generate unit test report
          echo "📊 Unit Test Coverage Report" > unit_test_report.md
          echo "============================" >> unit_test_report.md
          echo "" >> unit_test_report.md
          echo "**Target Coverage**: ${{ env.COVERAGE_THRESHOLD_UNIT }}%" >> unit_test_report.md
          echo "**Test Type**: Unit Tests" >> unit_test_report.md
          echo "**Date**: $(date -u)" >> unit_test_report.md
          echo "" >> unit_test_report.md

          if [ -f coverage-unit.xml ]; then
            # Extract coverage percentage from XML
            COVERAGE=$(python -c "
            import xml.etree.ElementTree as ET
            try:
                tree = ET.parse('coverage-unit.xml')
                root = tree.getroot()
                coverage = float(root.attrib.get('line-rate', 0)) * 100
                print(f'{coverage:.1f}')
            except:
                print('0.0')
            ")

            echo "**Actual Coverage**: ${COVERAGE}%" >> unit_test_report.md

            if (( $(echo "$COVERAGE >= ${{ env.COVERAGE_THRESHOLD_UNIT }}" | bc -l) )); then
              echo "**Status**: ✅ PASSED" >> unit_test_report.md
            else
              echo "**Status**: ❌ FAILED" >> unit_test_report.md
            fi
          else
            echo "**Status**: ❌ NO COVERAGE DATA" >> unit_test_report.md
          fi

      - name: Integration Test Coverage Validation
        if: github.event.inputs.test_level == 'integration_only' || github.event.inputs.test_level == 'comprehensive' || github.event.inputs.test_level == ''
        run: |
          echo "🔗 Running Integration Test Coverage Validation"

          # Create integration test structure if missing
          mkdir -p tests/integration

          # Run integration tests with coverage
          python -m pytest tests/integration/ \
            --cov=server \
            --cov-report=xml:coverage-integration.xml \
            --cov-report=html:htmlcov-integration \
            --cov-report=term \
            --cov-fail-under=${{ env.COVERAGE_THRESHOLD_INTEGRATION }} \
            -v || echo "Integration tests completed with coverage below threshold"

          # Generate integration test report
          echo "📊 Integration Test Coverage Report" > integration_test_report.md
          echo "===================================" >> integration_test_report.md
          echo "" >> integration_test_report.md
          echo "**Target Coverage**: ${{ env.COVERAGE_THRESHOLD_INTEGRATION }}%" >> integration_test_report.md
          echo "**Test Type**: Integration Tests" >> integration_test_report.md
          echo "**Date**: $(date -u)" >> integration_test_report.md

      - name: Functional Test Coverage Validation
        if: github.event.inputs.test_level == 'functional_only' || github.event.inputs.test_level == 'comprehensive' || github.event.inputs.test_level == ''
        run: |
          echo "🎯 Running Functional Test Coverage Validation"

          # Create functional test structure if missing
          mkdir -p tests/functional

          # Run functional tests with coverage
          python -m pytest tests/functional/ \
            --cov=server \
            --cov-report=xml:coverage-functional.xml \
            --cov-report=html:htmlcov-functional \
            --cov-report=term \
            --cov-fail-under=${{ env.COVERAGE_THRESHOLD_FUNCTIONAL }} \
            -v || echo "Functional tests completed with coverage below threshold"

      - name: E2E Foundation Matrix Validation
        if: github.event.inputs.test_level == 'comprehensive' || github.event.inputs.test_level == ''
        run: |
          echo "🏗️ Running E2E Foundation Matrix Validation"

          python -c "
          import asyncio
          import sys
          import os
          sys.path.append('tests/e2e')

          async def run_foundation_tests():
              try:
                  from test_foundation_matrix import run_comprehensive_test_matrix
                  results = await run_comprehensive_test_matrix()

                  print(f'✅ Foundation Matrix Tests Completed')
                  print(f'   Tests Run: {results.get(\"tests_run\", 0)}')
                  print(f'   Tests Passed: {results.get(\"tests_passed\", 0)}')
                  print(f'   Tests Failed: {results.get(\"tests_failed\", 0)}')

                  return results.get('tests_failed', 0) == 0

              except Exception as e:
                  print(f'❌ Foundation Matrix Tests Failed: {e}')
                  return False

          success = asyncio.run(run_foundation_tests())
          exit(0 if success else 1)
          "

      - name: Chaos Testing Validation
        if: github.event.inputs.test_level == 'chaos_testing'
        run: |
          echo "🔥 Running Chaos Testing Validation"

          python -c "
          import asyncio
          import sys
          import os
          sys.path.append('tests/chaos')

          async def run_chaos_tests():
              try:
                  from chaos_testing_suite import run_chaos_testing_suite
                  results = await run_chaos_testing_suite()

                  print(f'✅ Chaos Testing Completed')
                  print(f'   Scenarios Run: {results.get(\"tests_run\", 0)}')
                  print(f'   Scenarios Passed: {results.get(\"tests_passed\", 0)}')
                  print(f'   Scenarios Failed: {results.get(\"tests_failed\", 0)}')

                  return results.get('tests_failed', 0) == 0

              except Exception as e:
                  print(f'❌ Chaos Testing Failed: {e}')
                  return False

          success = asyncio.run(run_chaos_tests())
          exit(0 if success else 1)
          "

      - name: Coverage Validation
        if: github.event.inputs.test_level == 'coverage_validation' || github.event.inputs.test_level == 'comprehensive' || github.event.inputs.test_level == ''
        run: |
          echo "📊 Running Comprehensive Coverage Validation"

          python -c "
          import asyncio
          import sys
          import os
          sys.path.append('tests/coverage')

          async def run_coverage_validation():
              try:
                  from coverage_validator import CoverageValidator

                  validator = CoverageValidator()
                  results = await validator.run_comprehensive_coverage_validation()

                  overall = results.get('overall_assessment', {})

                  print(f'✅ Coverage Validation Completed')
                  print(f'   Overall Coverage: {overall.get(\"overall_coverage_percentage\", 0):.1f}%')
                  print(f'   Quality Gates: {overall.get(\"quality_gates_passed\", \"0/0\")}')
                  print(f'   Production Ready: {\"YES\" if overall.get(\"ready_for_production\", False) else \"NO\"}')

                  return overall.get('ready_for_production', False)

              except Exception as e:
                  print(f'❌ Coverage Validation Failed: {e}')
                  return False

          success = asyncio.run(run_coverage_validation())
          exit(0 if success else 1)
          "

      - name: Foundation SPEC Compliance Check
        run: |
          echo "🏛️ Checking Foundation SPEC Compliance"

          # Check implementation files for each foundation SPEC
          SPEC_007_FILES="server/contexts_unified.py server/context_ops_unified.py"
          SPEC_012_FILES="server/substrate_manager.py"
          SPEC_016_FILES=".github/workflows/"
          SPEC_020_FILES="server/memory/provider_registry.py server/memory/health_monitor.py server/memory/failover_manager.py server/memory/provider_security.py"
          SPEC_049_FILES="server/memory/sharing_contracts.py server/memory/consent_manager.py server/memory/temporal_access.py server/memory/audit_logger.py"

          echo "📋 Foundation SPEC Implementation Status" > spec_compliance_report.md
          echo "=======================================" >> spec_compliance_report.md
          echo "" >> spec_compliance_report.md

          # Check SPEC-007
          SPEC_007_STATUS="❌ MISSING"
          for file in $SPEC_007_FILES; do
            if [ -f "$file" ]; then
              SPEC_007_STATUS="✅ IMPLEMENTED"
              break
            fi
          done
          echo "- **SPEC-007 (Unified Context Scope)**: $SPEC_007_STATUS" >> spec_compliance_report.md

          # Check SPEC-012
          SPEC_012_STATUS="❌ MISSING"
          for file in $SPEC_012_FILES; do
            if [ -f "$file" ]; then
              SPEC_012_STATUS="✅ IMPLEMENTED"
              break
            fi
          done
          echo "- **SPEC-012 (Memory Substrate)**: $SPEC_012_STATUS" >> spec_compliance_report.md

          # Check SPEC-016
          if [ -d "$SPEC_016_FILES" ] && [ "$(ls -A $SPEC_016_FILES)" ]; then
            SPEC_016_STATUS="✅ IMPLEMENTED"
          else
            SPEC_016_STATUS="❌ MISSING"
          fi
          echo "- **SPEC-016 (CI/CD Pipeline)**: $SPEC_016_STATUS" >> spec_compliance_report.md

          # Check SPEC-020
          SPEC_020_COUNT=0
          for file in $SPEC_020_FILES; do
            if [ -f "$file" ]; then
              ((SPEC_020_COUNT++))
            fi
          done
          if [ $SPEC_020_COUNT -ge 3 ]; then
            SPEC_020_STATUS="✅ IMPLEMENTED"
          else
            SPEC_020_STATUS="⚠️ PARTIAL ($SPEC_020_COUNT/4 files)"
          fi
          echo "- **SPEC-020 (Memory Provider Architecture)**: $SPEC_020_STATUS" >> spec_compliance_report.md

          # Check SPEC-049
          SPEC_049_COUNT=0
          for file in $SPEC_049_FILES; do
            if [ -f "$file" ]; then
              ((SPEC_049_COUNT++))
            fi
          done
          if [ $SPEC_049_COUNT -ge 3 ]; then
            SPEC_049_STATUS="✅ IMPLEMENTED"
          else
            SPEC_049_STATUS="⚠️ PARTIAL ($SPEC_049_COUNT/4 files)"
          fi
          echo "- **SPEC-049 (Memory Sharing Collaboration)**: $SPEC_049_STATUS" >> spec_compliance_report.md

          echo "" >> spec_compliance_report.md
          echo "**Overall Foundation Status**: $(( (${SPEC_007_STATUS:0:1} == '✅') + (${SPEC_012_STATUS:0:1} == '✅') + (${SPEC_016_STATUS:0:1} == '✅') + (${SPEC_020_STATUS:0:1} == '✅') + (${SPEC_049_STATUS:0:1} == '✅') ))/5 SPECs Complete" >> spec_compliance_report.md

      - name: Quality Gate Enforcement
        run: |
          echo "🚪 Enforcing Quality Gates"

          # Initialize quality gate results
          UNIT_GATE_PASSED=false
          INTEGRATION_GATE_PASSED=false
          FUNCTIONAL_GATE_PASSED=false
          OVERALL_GATE_PASSED=false

          # Check unit test coverage
          if [ -f coverage-unit.xml ]; then
            UNIT_COVERAGE=$(python -c "
            import xml.etree.ElementTree as ET
            try:
                tree = ET.parse('coverage-unit.xml')
                root = tree.getroot()
                coverage = float(root.attrib.get('line-rate', 0)) * 100
                print(f'{coverage:.1f}')
            except:
                print('0.0')
            ")

            if (( $(echo "$UNIT_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_UNIT }}" | bc -l) )); then
              UNIT_GATE_PASSED=true
              echo "✅ Unit Test Quality Gate: PASSED ($UNIT_COVERAGE%)"
            else
              echo "❌ Unit Test Quality Gate: FAILED ($UNIT_COVERAGE% < ${{ env.COVERAGE_THRESHOLD_UNIT }}%)"
            fi
          else
            echo "❌ Unit Test Quality Gate: NO DATA"
          fi

          # Check integration test coverage
          if [ -f coverage-integration.xml ]; then
            INTEGRATION_COVERAGE=$(python -c "
            import xml.etree.ElementTree as ET
            try:
                tree = ET.parse('coverage-integration.xml')
                root = tree.getroot()
                coverage = float(root.attrib.get('line-rate', 0)) * 100
                print(f'{coverage:.1f}')
            except:
                print('0.0')
            ")

            if (( $(echo "$INTEGRATION_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_INTEGRATION }}" | bc -l) )); then
              INTEGRATION_GATE_PASSED=true
              echo "✅ Integration Test Quality Gate: PASSED ($INTEGRATION_COVERAGE%)"
            else
              echo "❌ Integration Test Quality Gate: FAILED ($INTEGRATION_COVERAGE% < ${{ env.COVERAGE_THRESHOLD_INTEGRATION }}%)"
            fi
          else
            echo "⚠️ Integration Test Quality Gate: NO DATA (treating as passed for now)"
            INTEGRATION_GATE_PASSED=true
          fi

          # Check functional test coverage
          if [ -f coverage-functional.xml ]; then
            FUNCTIONAL_COVERAGE=$(python -c "
            import xml.etree.ElementTree as ET
            try:
                tree = ET.parse('coverage-functional.xml')
                root = tree.getroot()
                coverage = float(root.attrib.get('line-rate', 0)) * 100
                print(f'{coverage:.1f}')
            except:
                print('0.0')
            ")

            if (( $(echo "$FUNCTIONAL_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_FUNCTIONAL }}" | bc -l) )); then
              FUNCTIONAL_GATE_PASSED=true
              echo "✅ Functional Test Quality Gate: PASSED ($FUNCTIONAL_COVERAGE%)"
            else
              echo "❌ Functional Test Quality Gate: FAILED ($FUNCTIONAL_COVERAGE% < ${{ env.COVERAGE_THRESHOLD_FUNCTIONAL }}%)"
            fi
          else
            echo "⚠️ Functional Test Quality Gate: NO DATA (treating as passed for now)"
            FUNCTIONAL_GATE_PASSED=true
          fi

          # Overall quality gate assessment
          if [ "$UNIT_GATE_PASSED" = true ] && [ "$INTEGRATION_GATE_PASSED" = true ] && [ "$FUNCTIONAL_GATE_PASSED" = true ]; then
            OVERALL_GATE_PASSED=true
            echo "✅ Overall Quality Gate: PASSED"
            echo "🎉 READY FOR EXTERNAL ONBOARDING"
          else
            echo "❌ Overall Quality Gate: FAILED"
            echo "⚠️ REQUIRES COVERAGE IMPROVEMENTS BEFORE ONBOARDING"
          fi

          # Set output for downstream jobs
          echo "QUALITY_GATE_PASSED=$OVERALL_GATE_PASSED" >> $GITHUB_ENV

      - name: Generate Comprehensive Test Report
        if: always()
        run: |
          echo "📊 Generating Comprehensive Test Report"

          cat > comprehensive_test_report.md << 'EOF'
          # SPEC-052: Comprehensive Test Validation Report

          **Workflow Run**: ${{ github.run_number }}
          **Trigger**: ${{ github.event_name }}
          **Branch**: ${{ github.ref_name }}
          **Date**: $(date -u)
          **Test Level**: ${{ github.event.inputs.test_level || 'comprehensive' }}

          ## 🎯 Quality Gate Results

          EOF

          # Add quality gate results
          if [ "${QUALITY_GATE_PASSED:-false}" = "true" ]; then
            echo "**Overall Status**: ✅ **PASSED** - Ready for external onboarding" >> comprehensive_test_report.md
          else
            echo "**Overall Status**: ❌ **FAILED** - Requires improvements before onboarding" >> comprehensive_test_report.md
          fi

          echo "" >> comprehensive_test_report.md
          echo "### Coverage Thresholds" >> comprehensive_test_report.md
          echo "- Unit Tests: ${{ env.COVERAGE_THRESHOLD_UNIT }}%" >> comprehensive_test_report.md
          echo "- Integration Tests: ${{ env.COVERAGE_THRESHOLD_INTEGRATION }}%" >> comprehensive_test_report.md
          echo "- Functional Tests: ${{ env.COVERAGE_THRESHOLD_FUNCTIONAL }}%" >> comprehensive_test_report.md
          echo "- Overall Target: ${{ env.COVERAGE_THRESHOLD_OVERALL }}%" >> comprehensive_test_report.md
          echo "" >> comprehensive_test_report.md

          # Add individual test reports if they exist
          if [ -f unit_test_report.md ]; then
            echo "## Unit Test Results" >> comprehensive_test_report.md
            cat unit_test_report.md >> comprehensive_test_report.md
            echo "" >> comprehensive_test_report.md
          fi

          if [ -f integration_test_report.md ]; then
            echo "## Integration Test Results" >> comprehensive_test_report.md
            cat integration_test_report.md >> comprehensive_test_report.md
            echo "" >> comprehensive_test_report.md
          fi

          if [ -f spec_compliance_report.md ]; then
            echo "## Foundation SPEC Compliance" >> comprehensive_test_report.md
            cat spec_compliance_report.md >> comprehensive_test_report.md
            echo "" >> comprehensive_test_report.md
          fi

          echo "## 📋 Next Steps" >> comprehensive_test_report.md
          echo "" >> comprehensive_test_report.md

          if [ "${QUALITY_GATE_PASSED:-false}" = "true" ]; then
            echo "✅ **System is ready for external onboarding**" >> comprehensive_test_report.md
            echo "- All quality gates passed" >> comprehensive_test_report.md
            echo "- Foundation SPECs have adequate test coverage" >> comprehensive_test_report.md
            echo "- Proceed with SPEC-058 Documentation Expansion" >> comprehensive_test_report.md
          else
            echo "⚠️ **System requires improvements before onboarding**" >> comprehensive_test_report.md
            echo "- Address failing quality gates" >> comprehensive_test_report.md
            echo "- Improve test coverage in identified areas" >> comprehensive_test_report.md
            echo "- Re-run validation after improvements" >> comprehensive_test_report.md
          fi

      - name: Upload Test Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: spec-052-test-reports-${{ github.run_number }}
          path: |
            comprehensive_test_report.md
            unit_test_report.md
            integration_test_report.md
            spec_compliance_report.md
            coverage-*.xml
            htmlcov-*
            test_coverage_validation_report.md
            chaos_testing_report.md

      - name: Upload Coverage Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports-${{ github.run_number }}
          path: |
            coverage-*.xml
            htmlcov-*

      - name: Comment PR with Results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## 🧪 SPEC-052 Test Validation Results\n\n';

            try {
              if (fs.existsSync('comprehensive_test_report.md')) {
                const report = fs.readFileSync('comprehensive_test_report.md', 'utf8');
                comment += report;
              } else {
                comment += '❌ Test validation report not generated\n';
              }
            } catch (error) {
              comment += `❌ Error reading test report: ${error.message}\n`;
            }

            comment += '\n\n---\n*This comment was automatically generated by the SPEC-052 Comprehensive Test Validation workflow.*';

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Fail Job if Quality Gates Failed
        if: env.QUALITY_GATE_PASSED != 'true'
        run: |
          echo "❌ Quality gates failed - blocking merge"
          echo "Please address the failing tests and coverage requirements before merging."
          exit 1
